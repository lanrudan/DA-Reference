---
title: "ABTest"
---

## **概览**

  本章介绍AB测试。**AB测试**（也称为随机对照试验 - RCT, Randomized Controlled Trial）是一种用于因果推断的实验设计方法。其核心在于通过**随机分配**将受试单元（用户、会话、页面访问等）分配到不同的处理组（通常是A：对照组/Baseline，B：新方案组），在控制其他变量（通过随机化）的前提下，比较不同处理对特定目标指标的影响，从而决定哪个更优。

  本章参考了\@knuth84

## **理论基础**

### 因果推断的基石：潜在结果框架与随机化

1.  **潜在结果框架（Neyman-Rubin Causal Model）：**

    -   **定义**：对于每个实验单元$i$（用户、会话等），定义两个潜在结果：

        -   $Y_i(1)$：单元$i$接受处理$B$（干预）时的结果

        -   $Y_i(0)$：单元$i$接受处理$A$（对照）时的结果

    -   **个体处理效应：**$\text{ITE} = \tau_i = Y_i(1) - Y_i(0)$

    -   **根本问题**：对于任何单元$i$，我们只能观察到$Y_i(1)$或$Y_i(0)$，永远无法同时观测两者，观察到的结果$Y_i^{obs} = Z_i * Y_i(1) + (1 - Z_i) * Y_i(0)$，其中$Z_i$是指示变量，$Z_i=1$ 表示分配到$B$组，$Z_i=0$ 表示分配到$A$组)。

    -   **目标**：估计**平均处理效应**$\text{ATE} = \tau = E[\tau_i] = E[Y_i(1)-Y_i(0)]=E[Y(1)]-E[Y(0)]$

2.  **随机化**

-   **关键假设（Ignorability/Unconfoundedness）**：通过**完全随机分配**，我们确保处理分配$Z_i$独立于潜在结果，即$Y_i(1),Y_i(0) \perp Z_i$，这意味着$E[Y_i(1) | Z_i=1] = E[Y_i(1) | Z_i=0] = E[Y_i(1)]$（同理于$Y_i(0)$）。

-   **无偏估计量：**基于观测数据，ATE 的一个简单无偏估计量是**组间均值差**$$\hat{τ}_{DiM} = \hat{E}[Y^{obs} | Z=1] - \hat{E}[Y^{obs} | Z=0] = \bar{Y}_B - \bar{Y}_A$$

    -   **证明无偏性**： $$E[\hat{τ}_{DiM}] = E[\bar{Y}_B - \bar{Y}_A] = E[\bar{Y}_B] - E[\bar{Y}_A] = E[Y(1) | Z=1] - E[Y(0) | Z=0]$$（根据观测数据定义）

        由于随机化（$Y_i(1),Y_i(0) \perp Z_i$），有$E[Y(1) | Z=1] = E[Y(1)]$且$E[Y(0) | Z=0] = E[Y(0)]$ 因此$E[\hat{τ}_{DiM}] = E[Y(1)] - E[Y(0)] = ATE$

-   **协变量平衡 (Covariate Balance):** 随机化也确保所有观测到的$X$ 和未观测到的 $U$ 混杂因素（Confounders）在组间的分布是相同的（渐近意义上）： $F(X | Z=1) = F(X | Z=0)$和$F(U | Z=1) = F(U | Z=0)$

这是实现$Y(1),Y(0) \perp Z$的关键保障。AA测试检查的就是观测到的 X是否平衡。

### 假说检验

1.  **核心框架**

-   原假设$H_0$：$ATE = 0$或等价的$\mu_A = \mu_B$（总体均值相等）或$p_A = p_B$（总体比例相等）。

-   备择假设$H_1$：$ATE \neq 0$（双尾）或$ATE>0$或$ATE<0$（单尾）。

-   检验统计量$T$：构造一个统计量，其分布在$H_0$成立时已知（或渐进已知）。

-   $P-value$：$p=P\left(|T| \geq \left|t_{obs}\right| \middle| H_0\right)$，即$H_0$成立时，观察到当前统计量$t_{obs}$甚至于更极端值的概率。

-   决策：若$p<\alpha$，$\alpha$为我们预设的显著性水平，则拒绝$H_0$。

2.  **连续性指标：双样本**$t$**检验**

  例如平均订单金额、会话时长。

-   假设：

    -   独立性Independent Samples

    -   正态性Normality：每组数据近似正态分布（或样本量足够大，依赖CLT）

    -   方差齐性Homoscedasticity：$\sigma_A^2 = \sigma_B^2 = \sigma^2$

-   检验统计量：

$$t = \frac{(\bar{Y}_B - \bar{Y}_A) - 0}{s_p \sqrt{\frac{1}{n_A} + \frac{1}{n_B}}}$$ 其中$s_p² = \frac{(n_A - 1)s_A² + (n_B - 1)s_B²}{n_A + n_B - 2}$是合并方差估计量 (Pooled Variance Estimator)，$s_A^2$，$s_B^2$是样本方差，$n_A$，$n_B$是样本量。

-   分布：在$H_0$和假设成立的条件下，$t \sim t_{df}$，自由度$df = n_A+n_B-2$

-   方差不齐 (Welch's t-test):若方差不齐，使用调整后的统计量和自由度， $$t = \frac{\bar{Y}_B - \bar{Y}_A}{\sqrt{\frac{s_A²}{n_A} + \frac{s_B²}{n_B}}}$$，自由度为$df ≈ \frac{(\frac{s_A²}{n_A} + \frac{s_B²}{n_B})²}{\frac{(s_A²/n_A)²}{n_A - 1} + \frac{(s_B²/n_B)²}{n_B - 1}}$

3.  **比例型指标：双样本比例检验（**$Z$**检验）**

  例如点击率、转化率。

-   假设：

    -   独立性

    -   大样本：$(n_A p_A > 5, n_A (1-p_A) > 5, n_B p_B > 5, n_B (1-p_B) > 5)$，确保二项分布近似正态。

-   检验统计量（基于合并比例）： $$Z = \frac{\hat{p}_B - \hat{p}_A}{\sqrt{\hat{p}(1-\hat{p}) (\frac{1}{n_A} + \frac{1}{n_B})}}$$ 其中$\hat{p} = \frac{X_A + X_B}{n_A + n_B}$，$X_A$，$X_B$是成功的次数，是$H_0(p_A =p_B = p)$下$p$的合并估计量。

-   分布：在$H_0$和假设成立的条件下，$Z\sim N(0, 1)$（渐近）。

-   基于独立方差（更常用且推荐）：

$$Z = \frac{\hat{p}_B - \hat{p}_A}{\sqrt{\hat{p}(1-\hat{p}) (\frac{1}{n_A} + \frac{1}{n_B})}}$$，这个版本不依赖与$H_0$下$p_A=p_B$的假设，在构建置信区间时更加自然，检验功效稍低但是更稳健。渐近分布同样是$N(0, 1)$。

4.  **计数型指标**   例如人均点击次数。常假设服从泊松分布，使用基于泊松回归或负二项回归的检验（处理过离散问题）。

5.  **置信区间（CI）**

-   概念：基于样本数据构造一个区间$(L, U)$，使得我们要估计的参数$\theta$（我们这里是$ATE$）落在这个区间的概率是$1-\alpha$，即$P(L\leq \theta \leq U) = 1-\alpha$。频率学派的解释是，重复多次试验，每次构造一个CI，那么大约$1-\alpha$的CI会包含真实的$\theta$。以下均以双尾检验为例。

-   连续性指标（差异）： $$(\bar{Y}_B - \bar{Y}_A) \pm t_{df, 1-\alpha/2} \times SE(\bar{Y}_B - \bar{Y}_A)$$ 其中 $SE(\bar{Y}\_B - \bar{Y}\_A) = \sqrt{\frac{s_A²}{n_A} + \frac{s_B²}{n_B}}$（Welch)）或$s_p \sqrt{\frac{1}{n_A} + \frac{1}{n_B}}$（Pooled）。

-   比例型指标（差异）：$$(\hat{p}_B - \hat{p}_A) \pm Z_{1-\alpha/2} \times \sqrt{\frac{\hat{p}_A(1-\hat{p}_A)}{n_A} + \frac{\hat{p}_B(1-\hat{p}_B)}{n_B}}$$

-   解读：如果95%CI不包含0，则双尾验证在\$\alpha \$显著性水平下显著。CI宽度反映了估计的精确度。

### 统计功效与样本量计算

1.  定义与公式

-   I类错误（$\alpha$）：$H_0$为真时，错误地拒绝$H_0$。也就是说宣称有差异实际没有。

-   II类错误（$\beta$）：$H_1$为真时，没有拒绝$H_0$。也就是说实际上有差异但是没有检测到。

-   功效（Power）$1-\beta$：$P(\text{Reject } H₀ | H₁ \text{ is true})$。即当真实效应$\delta = |ATE|$（或$|\mu_B -\mu_A|$，$|p_B - p_A|$）存在且等于最小期望检测效应（MDE）时，正确拒绝$H_0$的概率。**换句话说，就是当处理B确实比A好（**$H_1$为真）时，正确拒绝$H_0$的概率$1-\beta$。

-   影响功效的关键因素（以双样本$t$检验为例）：

    -   效应量（$\delta$）:期望检测到的最小有意义的差异$\delta = \frac{|\mu_B - \mu_A|}{\sigma}$，这是标准化效应量，Cohen'sd。$\delta$越大，功效越高，$\delta$越小，检测所需要的样本量越大。

    -   样本量（$n = n_A= n_B$）:$n$越大，功效越高，功效$\propto \sqrt{n}$

    -   显著性水平（$\alpha$）：犯I类错误（假阳性）的概率。$\alpha$越大，即允许更多假阳性，功效越高。$\alpha$越小，所需要样本量越大。

    -   方差（$\sigma^2$）：$\sigma^2$越大，功效越低。指标本身的变异性越大，检测相同效应量所需的样本量越大（对于连续型指标）。

    -   基准比率（p_A）：对于比例指标，基准转化率影响方差（p(1-p)），在p=0.5时最大。

-   样本量计算公式（双样本$t$检验，双尾，等样本量）：

$$n \approx \frac{2 (Z_{1-\alpha/2} + Z_{1-\beta})^2 \sigma^2}{\delta^2}$$

其中$Z_{1-\alpha/2},Z_{1-\beta}$是标准正态分布的分位数，$\delta$是未标准化的效应量($|\mu_B-\mu_A|$)。这个公式推导基于$H_0$下的统计量$\sim N(0, 1)$，$H_1$下的统计量$\sim N(\frac{\delta}{\sigma \sqrt{2/n}}, 1)$，令两个分布的拒绝域边界相交即可解出$n$。

-   比例型指标（双样本比例检验，双尾，等样本量）：

$$n \approx \frac{ (Z_{1-\alpha/2} \sqrt{2\bar{p}(1-\bar{p})} + Z_{1-\beta} \sqrt{p_A(1-p_A) + p_B(1-p_B)} )² }{\delta²}$$ 其中$\delta = |p_B-p_A|,\bar{p} = \frac{p_A+p_B}{2}$，推导类似，考虑$H_0$下方差基于$\bar{p}$，$H1$下方差基于真实的$p_A$和$p_B$。

### 方差估计的挑战（用户级随机化）

-   问题核心：在用户级随机化的AB测试中，分析单元（如页面浏览PV，点击Click）与随机化单元（用户User）不一致。同一个用户的不同事件/行为不独立。

-   后果：传统的方差估计公式（如$\frac{s^2}{n}$）会严重低估真实方差，导致：

    -   CI过窄

    -   $p-value$过小，I类错误率（假阳性率）膨胀

-   数学解释：

    -   假设有$m$个用户$i = 1...m$，用户$i$贡献$n_i$个观测值（如$PV$），总观测值$N = \sum_i n_i$

    -   核心指标$Y$在用户$i$上的平均为$\bar{Y}_i$

    -   组件差异估计量$\hat{\tau} = \bar{Y}_B - \bar{Y}_A$

    -   其真实方差为：$Var(\hat{τ}) = Var(\frac{1}{m_B} \sum_{i \in B} \bar{Y}_i - \frac{1}{m_A} \sum_{j \in A} \bar{Y}_j)$，由于用户间是独立的，$Var(\hat{τ}) = \frac{Var(\bar{Y}_i | B)}{m_B} + \frac{Var(\bar{Y}_j | A)}{m_A}$

    -   而传统方差估计（假设观测独立）为：$\widehat{Var}_{naive}(\hat{τ}) \approx \frac{s_B²}{N_B} + \frac{s_A²}{N_A}$，其中$N_B = \sum_{i \in B} n_i$（B组总观测数），$s_B^2$是基于所有B组观测值计算的方差

    -   $\widehat{Var}_{naive}$会系统性小于$Var(\hat{\tau})$，因为忽略了用户的内相关性。低估的程度取决于用户内相关性的强度和用户行为次数$n_i$的变异度。

-   解决方案：

    -   $Delta$ $Method$：推倒$\hat{\tau}$方差的理论表达式并进行估计。适用于特定指标类型（如人均指标）。

    -   聚类标准误（Cluster-Robust Standard Errors - CRSE）：将方差估计建立在随机化单元（用户）的层面。

        -   将每一个用户视为一个“聚类”

        -   计算每个用户$i$对$\hat{\tau}$的“得分”（Influence Function）或残差贡献$e_i$

        -   聚类标准误为： $\widehat{Var}_{CR}(\hat{τ}) = \frac{m}{m-1} \frac{m}{m_A m_B} \sum_{i=1}^m (\tilde{e}_i - \bar{\tilde{e}})^2$，其中$\tilde{e}_i$是用户$i$的聚合残差贡献（具体形式取决于模型）。这是最常用且稳健的方法。

    -   Booststrap（聚类Booststrap）：对用户进行重抽样（而不是对观测值），保持用户的所有数据完整。每次重抽样后计算$\hat{\tau}$，用多次（B次）重抽样得到的$\hat{\tau}^{(b)}$的方差来估计$\hat{\tau}$。计算量大但是灵活。

### 多重检验问题（Multiple Testing Problem）

-   问题核心：同时进行$K$次独立的假设检验，每个检验的显著性水平都为$\alpha$。那么至少出现一次假阳性（I类错误）的概率（族系错误率FWER,Family-Wise Error Rate）是$$FWER = P(\text{至少一个错误拒绝} | \text{所有 } H₀ \text{ 为真}) = 1 - (1 - α)^K ≈ Kα \quad (\text{当 } α \text{ 很小时})$$，即使$\alpha = 0.05$，当$K$为10时，FWER约为0.4，假阳性率非常高。

-   ABTest中的来源：

    -   同时测试多个核心指标（$K>1$）

    -   同时测试多个变体（A/B/C/D...测试，$K>1$个比较）

    -   数据窥探（Peeking）：在实验运行期间多次查看结果并进行检验。每次查看都是一次独立的检验机会（$K$很大）。

-   控制方法（待补充）

### 进阶统计视角（待补充）

  AB测试的统计学原理深植于**因果推断**的潜在结果框架，**随机化**是其无偏估计的基石。**假设检验 (t/Z检验)**提供决策依据，**置信区间**量化不确定性。**功效分析**确保实验可靠性，**样本量计算**是实验设计的核心。实践中必须警惕**方差低估 (用户级相关性)** 和**多重检验膨胀**，采用聚类标准误或校正方法。贝叶斯方法和序贯检验提供了替代视角和效率提升。理解异质性效应能挖掘更深价值。掌握这些原理，是设计和解读可靠AB测试的关键。

## **AB测试的流程与步骤**

  ABtest其实就是控制变量法。为了评估测试和验证模型/项目的效果，在app/pc端设计出多个版本，在同一时间维度下，分别用组成相同/相似的群组去随机访问这些版本，记录下群组的用户体验数据和业务数据，最后评估不同方案的效果决定是否上线。

### 1.明确目标与假设

-   定义清晰的业务目标（如提升注册转化率、增加平均订单价值）。

-   提出具体的、可证伪的统计假设（$H_0$和$H_1$）。

-   确定期望检测的最小有意义效应量MDE

### 2.确定目标指标与护栏指标

-   **核心指标（Primary Metrics）：**直接反映实验目标的1-2个关键指标（如转化率、收入）
    -   例如：修改购买页面的主色调能够帮助用户购买率提升3%

    -   那么用户购买率就是我们关注的

    -   还需要关注一些新策略是否会对其他重要指标产生负面影响
-   **护栏指标（Guardrail Metrics）：**监控实验潜在负面影响的指标（如页面加载时间、关键功能使用率、崩溃率）。确保优化核心指标不以牺牲其他重要方面为代价。
    -   Organization Guardrail Metrics：新功能可能好但是加载慢；该付款界面UI对用户的下单单价不能有影响

    -   Trustworthy-related metrics：比如检查randomization，要用T-test或者卡方检验在查看A/B组的其他特征一致性
-   **探索性指标（Exploratory Metrics）**：帮助理解用户行为变化，发现意外结果的辅助指标。

### 3.实验设计

-   单元选择（Unit of Diversion/randomization Unit）：决定随机化的最小单元（用户ID，设备ID，会话ID，页面访问）。选择时需要考虑指标计算（用户级指标需用户级分流）、用户多次体验（保持体验一致性）、样本独立性（避免干扰）。用户级分流最常见。

-   分流比例（Traffic Allocation）：分配多少样本去A/B组。通常是50:50，这样统计效率最高。但也会根据风险、流量大小调整（如90:10，新方案流量少）。需要确保样本量足够。

-   样本量计算（Sample Size Estimation）：基于$\alpha, \beta, MDE$，基准值（$p_A$或$\sigma_A$）计算每组所需要的最小样本量/实验时长。这一点至关重要！样本不足会导致功效低（II类错误风险高），样本过大浪费资源和时间。

-   分层与区组（Sractification&Blocking）：在随机化钱按重要特征（如国家、设备类型、用户新老）分层或区组，确保这些特征在组间均衡分布，提高统计精度（尤其当特征与指标强相关时）。

-   触发与曝光（Triggering&Exposure）：明确那些用户会被纳入实验（所有的访问者？特定路径访问者？）。以及何时记录他们被“曝光”于实验条件、确保分析对象是真正“看到了”实验变化的用户，称为“曝光后分析”。

  下面将具体举例进行介绍。

-   **WHO**

  AB实验实验需要控制变量，确保两组中只有一个不同的变量，其余变量一致。

-   必须满足的条件：

    -   特征相同或相似的用户群组

    -   同一时间维度

-   操作方法：

    -   利用用户唯一标识的尾号或者其他标识进行分类，如奇偶分为两组

    -   用一个hash函数将用户的唯一标识进行取模，分桶。可以将用户均匀地分到若干桶中，如分到100/1000个桶中，这样的好处就是可以进一步将用户打散，提高分组的效果。

-   当然，如果有多个分组并行的情况的话，要考虑独占域和分享域问题。独占域指不同域之间的用户相互独立，交集为空。对于共享域，我们要进行分层。但是在分层中，下一层要将上一层的用户打散，确保下一层用户的随机性。

-   实验对象：

    -   如果是双边市场的话，可以从用户、平台、生产者三方进行实验

        -   双边市场：指平台同时服务两类或多类相互依存的客户。例如，淘宝连接了买家和卖家，滴滴连接了乘客和司机，内容平台连接了用户和创作者。

        -   信息流实验：指的是围绕“信息”在用户和生产者之间流动的过程所进行的实验。推荐系统是其中的核心组成部分。

        -   用户（C端实验）

            -   是最常见的AB实验场景，直接面向用户

            -   产品策略：例如新功能上线，用户界面UI改版

            -   运营策略：比如推送通知的文案、活动页面设计

            -   客户端改版：例如APP的新版本、页面布局的调整

            -   核心目标：提升用户体验，提高留存度和活跃度

        -   cp/内容生产者（B端实验）

            -   面向内容生产者或者商家的产品和策略

            -   例如：内容池优化（让创作者更方便地上传内容）、创作者激励（新的分成机制）、黑产打击（识别和处理恶意内容）。

            -   旨在提升B端的生产效率、积极性，维护平台生态健康

        -   推荐实验（平台侧）

            -   面对：链接用户和CP的关键桥梁---推荐算法

            -   例子：自动训参（Auto-tuning，自动调整推荐算法的参数）、召回优化（提高找到用户可能喜欢的内容的效率）、排序优化（更好的对召回内容进行排序）、多目标优化（同时考虑点击率、停留市场、GMV等多个指标）、体验优化（减少广告干扰）、人群策略（针对特定人群提供定制化推荐）。推荐算法的效果直接影响着用户和CP两段的满意度。

    -   推荐实验常用试验方法：（未展开，有机会可以展开）

        -   ABTest

        -   上线实验

        -   MAB实验

        -   在线寻参

-   人群圈选方式（行为圈选、标签圈选、属性圈选）

    -   多层方案

        -   分流：用户分流是指按照地域、性别、年龄等等将用户均匀地分成几个组，1个用户只能出现在1个组中。

            -   适合互斥实验：实验在同一层拆分流量，且无论怎么拆分，不同组的流量是不会重叠的

            -   例如“安卓用户”，“只看北京用户”。很多情况中不同城市的用户的的情况会有很大差别

            -   问题：实际情况中，往往会上线多个实验。例如可能同时上线样式形态、广告位置策略、预估模型的实验。如果只按照分流模式来说，在每组实验放量10%的情况下，整体的流量只能同时开展10个实验。效率很低。为了解决这个问题，提出了用户分层、流量复用的方法。

        -   分层：同一份流量可以分布在多个实验层，也就是说**同一批用户可以出现在不同的实验层**，前提是**各个实验层之间无业务关联**，保证**这一批用户都均匀地分布到所有的实验层中**，达到用户正交的效果就可以，从而让实验流量复用。

            -   根据业务方人为定义一些分层--如：UI层、推荐算法层

            -   每一层对用户随机分组：流量经过每一层时都会被打散重新分配，下一层的每一组的流量都随机来源于上一层各组的流量

            -   实现同一个用户出现在不同层的不同组中，流量重复利用

        -   分流分层模型：在此模型中增加组、层，并且可以相互嵌套。要求与实际的业务相匹配，拆分过多的结构可能会把简单的业务复杂化，拆分过少的结构又可能不满足实际业务。

  上图就是分流分层模型的一个简单的图示。我们有：域1+域2=100%流量，B1层=B2层=B3层=域2流量，(B1-1)+(B1-2)+(B1-3)=B1层流量

-   规则：

    -   域1和域2拆分流量，即域1域2互斥

    -   流量流过域2中的B1层、B2层、B3层时，B1层、B2层、B3层的流量都与域2的流量相等，此时B1层、B2层、B3层的流量是正交的

    -   流量流过域2中的B1层时，又把B1层分为了B1-1，B1-2,B1-3，此时B1-1，B1-2,B1-3又是互斥的

-   使用场景：

    -   例1：B1层、B2层、B3层可能分别为：UI层、搜索结果层、广告结果层，这基层基本上没有业务关联，即使共用相同的流量（流量正交）也不会对实际的业务造成结果。

    -   但是如果不同层之间所进行的实验相互关联，如B1层是修改的一个页面的按钮文字颜色，B2层时修改的按钮的颜色，当按钮文字颜色和文字颜色一样时，该按钮以及不可用了。因此建议同一类型的实验在同一层内进行，并且需要考虑到不同实验互相的依赖。

    -   例2：域1的此种分流意义在于：如果我们希望其他任何实验都不能对我们的实验造成干扰，保证最后实验的可信度。

-   B端实验

    -   内容池：

        -   业务策略：低质过滤、热点运营、内容引入、内容使用天数

        -   模型更新：tag更新、清晰度调整、封面图调整、评分体系更新

    -   内容创作者：

        -   业务策略：账号过滤、账号提权、账号引入、分润策略

        -   模型更新：评级模型更新、黑产打击模型更新

-   **what**——选物料

  明确改动点——话术模版/H5/图片素材，保证是单一因素

-   **where**——选渠道

  短信/外呼/邮件/站内私信

-   **when**——选时机

  立即出发/定时触达/例行触达/事件触发

  注意：实验时长要与产品的“数据特征周期”一致

  例如：直播类app产品，用户在周一到周五的活跃度较低，在周末活跃度高，以一个自然周为周期，不断循环。那么该产品的实验的时长应设置为一周。

-   **How Large**——决定样本量

    -   在前面有介绍过具体公式，这里简化为$N  \approx \frac{16 \times \sigma^2}{\delta^2}$

        -   样本标准差$\sigma$衡量了整体样本数据的波动性

            -   观测指标为绝对值类指标时：$\sigma^2 = \frac{\sum_i^n(x_i-\bar{x})^2}{n-1}$

            -   观测指标为比率类指标时：$\sigma^2 = p_A(1-p_A)+p_B(1-p_B)$，$p_A,p_B$为观测数据，比如希望点击率从20%提升到25%，那么$p_A=0.2,p_B=0.25,\delta = 5%$

        -   组间预期差值$\delta$代表预期实验组和对照组两组数据的差

        -   这里我们取$\alpha = 0.05,\beta=0.2$，这里体现了AB实验的保守理念：低的$\alpha$，尽量避免在$H_0$为真的时候拒绝它，；高的$\beta$，可以适当的接受假的$H_0$，也就是说改进实际游泳但是不采取；总结下来就是宁肯砍掉多个号的产品，也不应该让任何不好的产品上线。

            ::: callout-note
            为什么我们需要计算最小样本量？

            理论上样本量肯定是越大越好。实际上，样本量是越小越好，因为

            -   流量有限：小公司就这么点流量，还要精打细算做各种测试，开发各种产品。在保证样本分组不重叠的基础上，产品开发速度会大大降低。

            -   试错成本大：如果拿50%的用户做实验，一周以后发现总收入下降了20%，这样一周时间的实验给公司造成了10%的损失，这样损失未免有点大。

            所以也可以应用一个流量大小Trick——RAMP-UP PLAN or 灰度测试：初始阶段,先分配较少的流量（如1%）进入实验，初始实验如果一切正常, 进一步加大流量，初始实验如果出现异常, 随时可以终止实验
            :::

    -   理论基础——CLT

        -   当样本量足够大的时候，样本均值$\bar{x}$的分布近似服从正态分布。

        -   当$H_0$为真的时候 ，实验组的样本均值$\bar{x}$服从均值为$\mu_t = \mu_c$，方差为$\frac{\sigma^2}{n}$的正态分布。

        标准化后的$Z$统计量：$Z=\frac{\bar{x}-(\mu_c-\mu_t)}{\sqrt{2\sigma^2/n}}$

        第二类错误是指在H0为假的时候，我们错误地接受了H0，也就是说Z统计量落在了接受域内。

$$\beta = P(|\frac{\bar{x}}{\sqrt{2\sigma^2/n}}| \le Z_{\alpha/2}) = P(-Z_{\alpha/2} \le \frac{\bar{x} - (\mu_c - \mu_t)}{\sqrt{2\sigma^2/n}} \le Z_{\alpha/2})$$

  由于中间部分服从标准正态分布，所以这个概率可以用正态分布的累计函数$\Phi$来表示：

$$\beta= \Phi(Z_{\alpha/2} - \frac{\mu_c - \mu_t}{\sqrt{2\sigma^2/n}}) - \Phi(-Z_{\alpha/2} - \frac{\mu_c - \mu_t}{\sqrt{2\sigma^2/n}})$$   不失一般性，我们假设$\mu_c>\mu_t$是一个比较大的整数，所以其中第二项$\Phi(-Z_{\alpha/2} - \frac{\mu_c - \mu_t}{\sqrt{2\sigma^2/n}})$的值会非常小，接近与0，因此可以忽略。与此同时，我们知道$\beta = \Phi(-Z_{\beta})$，最终可以得到$$n \approx \frac{2(Z_{\alpha/2} + Z_\beta)^2 \sigma^2}{(\mu_c - \mu_t)^2}$$。其中$\delta = |\mu_t-\mu_c|$是我们希望检测到的最小可检测效应MDE。$\sigma^2$是指标的方差，通常需要预估或者根据历史数据计算。

  以上公式是计算出单个实验组所需的样本量，若有多个实验组，乘以实验组的个数就可以得到最终的样本量；样本量也可以是一段时间里累计的样本量，比如需要10000个样本，每天1000个，累计10天也是可以的。

-   举例：

    -   对于绝对值指标：

        -   某商品详情页平均停留时长的标准差是20s，优化了商品详情页面后，预估至少有5s的提升，AB测试每个组需要的最少样本量：$\sigma = 20,\delta=5$，每个组所需要的最少的样本量就是16\*20**2/5**2=256

    -   比率类指标：

        -   某商品详情页点击率20%，优化后预期点击率提升到25%，每个组需要的最少样本量：16*(0.2*0.8+0.25\*0.75)/(0.25-0.2)\*\*2

-   在线计算工具：[Evans awesome AB Tools](https://www.evanmiller.org/ab-testing/)

-   代码（基于$delta$ $method$）:$\sqrt{n}(g(Y_n) - g(\theta)) \xrightarrow{d} N(0, \sigma^2 g'(\theta)^2)$

```{r, echo=FALSE, warning=FALSE}
library(reticulate)
use_condaenv("mineenv", required = TRUE)
```

```{python}
import numpy as np
import math
from sklearn.linear_model import MultiTaskLasso, Lasso
import pandas as pd
import scipy.stats
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler

```

1.  用卡方检验检验分流的均匀性

-   x 和 y：通常代表实验组和对照组的用户总数（或流量总数）。代码中的 31188, 31188 表示两组的流量是相等的。

-   x_target 和 y_target：通常代表实验组和对照组中某个特定事件的发生次数，例如转化、点击等。代码中的 3461, 3423 是两组的转化次数。

-   如果p-value非常小（比如小于0.05），说明两组的流量分布存在显著差异，可能分流有问题，实验结果不可信。

```{python}
from scipy.stats import chi2_contingency
def chi_test(x,y,x_target,y_target):
    kf_data = np.array([[x,x_target], [y,y_target]])
    kf = chi2_contingency(kf_data)
    print('chi-sq=%.4f, p-value=%.4f, df=%i expected_frep=%s'%kf)

chi_test(31188,31188,3461,3423)
```

2.  德尔塔方法计算方差

-   计算两个随机变量之比的方差。在ABTest中，CTR = 点击数/曝光数，CVR = 转化数/点击数，这两个指标本质上都是两个随机变量的商。

-   问题背景：传统的二项分布方差公式适用于伯努利试验（例如每个用户点击或者不点击）。但是对于比率指标，分母本身也是一个随机变量。这时使用简单的二项分布公式来估算方差就不正确了。

-   推导：假设我们需要计算$R = Y/X$的方差。通过一阶泰勒展开近似，得到$Var(\hat{R}) = Var\left(\frac{\bar{Y}}{\bar{X}}\right) \approx \left(\frac{\partial g}{\partial x}\right)^2 Var(\bar{X}) + \left(\frac{\partial g}{\partial y}\right)^2 Var(\bar{Y}) + 2\left(\frac{\partial g}{\partial x}\right)\left(\frac{\partial g}{\partial y}\right)Cov(\bar{X}, \bar{Y})$，其中$g(x,y) = \frac{y}{x},  \frac{\partial g}{\partial x} = -\frac{y}{x^2}, \frac{\partial g}{\partial y} = \frac{1}{x}$，代入得到$\Rightarrow Var(\hat{R}) \approx \frac{E[Y]^2}{E[X]^4} Var(\bar{X}) + \frac{1}{E[X]^2} Var(\bar{Y}) - \frac{2E[Y]}{E[X]^3} Cov(\bar{X}, \bar{Y})$

```{python}
def var_dtm(y_mean, x_mean, var_y, var_x, xy_mean):
    #计算协方差
    def cova(x_mean,y_mean,xy_mean):
        return xy_mean-x_mean*y_mean
    covar = cova(x_mean,y_mean,xy_mean)
    
    #商的variance
    var_ratio = var_y/x_mean**2 + var_x*y_mean**2/x_mean**4 - 2*covar*y_mean/x_mean**3
    return var_ratio
  
  
#发布器头图的 曝光数 > 发布数 为例
var = var_dtm(y_mean = 0.035214194, x_mean = 2.794877088302219
                ,var_y = 0.073726526, var_x = 185.240175
                    , xy_mean = 0.410843231521)


#实际的方差 有可能比二项分布折算方差会大很多
print('二项方差：',round(0.012599/(1-0.012599),4))
print('实际方差：',round(var,4))

```

3.  估算样本量

```{python}
'''
mean:对照组指标的均值（如基准转化率）
var：指标的方差，代码在调用时会分别使用“二项分布方差”和“德尔塔方法方差”。
lift_rate：预期的最小可检测效应\delta
'''
def sample_size(mean = 0.01259955,var = 0.0122,lift_rate = 0.025,alpha = 0.05,beta = 0.2):
    import math
    
    lift = lift_rate * mean
    num = var*(scipy.stats.norm.ppf(1-alpha)+scipy.stats.norm.ppf(1-beta))**2/(lift**2)
    return math.ceil(num)

print('采用二项方差的样本量估计：', sample_size(var = 0.0128))
print('采用deltaMethod的样本量估计：',sample_size())
```

### 4.实施与随机化，数据收集与监控

-   开发并部署实验变体（B）

    -   创建变体：对网站原有版本的元素进行所需要更改。可能是更改颜色，交换页面上元素是顺序，隐藏导航元素或完全自定义的内容。

-   构建可靠的随机化引擎，确保分配是真正的随机且不可预测的。常用方法：用户ID哈希取模、伪随机数生成器（需要确保seed独立）。

-   记录分配日志（用户ID，时间戳、分配组、实验ID）（埋点）

    -   这里要注意辛普森悖论！要严格执行之前设计的分流分层方案让样本均匀随机。

-   收集核心指标、护栏指标、探索性指标的数据（埋点采集数据）

-   实时/准实时监控：

    -   样本量积累情况

        -   比如实验组和对照组的分流的流量是否均匀

    -   核心指标的点估计和置信区间变化

    -   护栏指标是否有异常波动（设置警报阈值）

    -   检查AA测试是否通过

    -   观察对于用户的行为埋点是否埋的正确

### 5.数据分析

-   数据准备：清洗数据，处理异常值，确认分析窗口（如实验开始后稳定期的数据）。

-   AA测试/平衡检验：在分析AB结果之前，先检查实验组（A）和对照组（A）在**已知不受实验影响**的指标上（如实验前历史数据、用户属性、分流前行为）是否存在显著差异。若显著，表明随机化可能失败或者数据有问题，需要排查。

-   效应估计：计算核心指标的组间差异$\bar{y}_B-\bar{y}_A$或者$\hat{p}_B-\hat{p}_A$

-   统计显著性差异：使用合适的检验方法（t检验、Z检验、回归）计算P值

| 指标类型 | 检验方法 | 公式 | 适用场景 |
|:-----------------|:-----------------|:------------------|:-----------------|
| **连续型指标**<br>(人均时长、客单价) | **双样本t检验** | $t = \frac{\bar{Y}_A - \bar{Y}_B}{\sqrt{\frac{s_A^2}{n_A} + \frac{s_B^2}{n_B}}}$<br>$df \approx \frac{(\frac{s_A^2}{n_A} + \frac{s_B^2}{n_B})^2}{\frac{(\frac{s_A^2}{n_A})^2}{n_A-1} + \frac{(\frac{s_B^2}{n_B})^2}{n_B-1}}$ | 收入、时长等平滑分布数据 |
| **比例型指标**<br>(转化率、点击率) | **双样本Z检验** | $Z = \frac{\hat{p}_A - \hat{p}_B}{\sqrt{\hat{p}(1-\hat{p})(\frac{1}{n_A} + \frac{1}{n_B})}}$<br>$\hat{p} = \frac{X_A + X_B}{n_A + n_B}$ | 转化率、留存率等二分类数据 |
| **计数型指标**<br>(人均点击次数) | **负二项回归**<br>(处理过离散) | $\log(\mu_i) = \beta_0 + \beta_1 \cdot \text{Treatment}_i$<br>$\text{Var}(Y_i) = \mu_i + \alpha\mu_i^2$ | 用户行为次数（方差\>均值） |

::: callout-note
Q：为什么转化率、点击率是二分类的数据？

A：然我们常把转化率、点击率表述为百分比（如5.2%），但它们的底层数据本质是二分类数据（Binary Data）。原因在于构成这些指标的每一个最小观察单元只有两种互斥状态：

-   点击率CTR点击次数/曝光次数：点击记录为1，不点击记录为0

-   转化率CVR注册次数/进入落地页人数：注册记录为1，不点击记录为0

-   二分类数据违背连续型检验的核心假设：

    | 假设       | 连续型变量     | 二分类数据               |
    |------------|----------------|--------------------------|
    | 数据分布   | 近似正态分布   | Bournulli分布（0-1分布） |
    | 方差稳定性 | 方差与均值无关 | 方差依赖均值\$p(1-p)\$   |
    | 数值范围   | 无边界限制     | 数值被压缩在\[0,1\]      |

-   案例：广告点击率AB测试 (A组 vs B组)

    -   A组：曝光 10,000 次 → 点击 600 次 → $\hat{p}_A = 0.06$

    -   B组：曝光 10,000 次 → 点击 750 次 → $\hat{p}_B = 0.075$

    -   若错误使用t检验：

        -   计算两组的点击率（0.06vs0.075）的t统计量，得出p\<0.05

        -   问题：忽略数据的本质是离散二分类，且方差p(1-p)在p=0.5时最大，导致检验失真

    -   正确使用Z检验：

        -   $Z = \frac{\hat{p}_B - \hat{p}_A}{\sqrt{\hat{p}(1-\hat{p}) \left( \frac{1}{n_A} + \frac{1}{n_B} \right)}}, \quad \hat{p} = \frac{600+750}{10000+10000} = 0.0675$

        -   得到Z=5.0,p\<0.000，结论可靠
:::

::: callout-note
Q：什么时候可以近似使用t检验？

A：当同时满足以下两个条件时，二分类数据近似生态分布，可以谨慎使用t检验

-   样本量足够大：$n \cdot p >5$且$c\cdot(1-p)>5$

-   比例值远离边界：$0.2<p<0.8$，避免p接近0或1导致分布高度偏态

当然，互联网场景中，转化率等常不满足，严格使用比例检验或逻辑回归。
:::

-   CI构建：计算效应量的CI

-   多重检验校正：如果同时考虑多个核心指标或者同一个指标的多个变体（A/B/n测试），需要进行校正，以控制整体错误发现率（FDR）或族系错误率（FWER）。容易忽视但是很重要！

-   效应量解读：结合实际业务解读统计学差异是否显著。一个统计学上显著但是效应量微小的结果可能没有实际意义。

### 6.决策

-   显著且正向：发布B方案

-   显著但负向：放弃B，分析原因

-   不显著：

    -   检查功效：样本量是否足够？效应量是否小于MDE？

    -   检查实验设计：随机化是否有效？指标定义是否合理？触发逻辑是否正确？

    -   结论：在当前条件下，未检测到B方案与A方案有统计显著差异。这**不**等于证明两者效果相同（可能是II类错误）。

-   考虑结果在实际流量下的稳健性（如网络效应、长期效应）

::: callout-note
Q：如果你发现你在AB测试当中所选取的指标在**统计上来说都是不显著的**，你该怎么去判断这个实验的收益？

A：

-   在当前条件下，未检测到B方案与A方案有统计显著差异。这**不**等于证明两者效果相同（可能是II类错误）

-   但进一步，我们可以将这个指标去拆分成每一天去观察，如果指标的变化曲线每一天实验组都高于对照组，即使他在统计上来说是不显著的，我们也认为在这一个观测周期内，实验组的关键指标表现是优于对照组，并得出优化上线的结论。
:::

::: callout-note
Q：如果你在AB测试中发现**实验组核心指标明显优于对照组**，那这个优化就一定能够上线吗？

A：

-   不一定。举个例子，比如说有的时候我们想要提升产品的视觉展现效果。但是这种优化可能是以用户等待内容展现的时间作为代价来进行提升的。所以一个方面的优化可能会导致另一个方面的劣化。在做这个优化的时候，可能会对其他部门产生一些负向的影响，进而导致公司收入的下降。

-   所以，我们在进行AB测试的时候，必须要综合评估所有方面的一些指标变动，同时对于收益和损失来做一个评估，才能确认这个优化可以最终上线。
:::

## **挑战与陷阱**

  其实常见的错误总得来说就是两类，一类是弃真，一类是存伪。

  **弃真**是指实验组和对照组没有显著差异，但是我们接受了新方案。减少这种错误的方法就是提高显著性水平$\alpha$，比如p小于0.05才算显著，而不是选用0.1   **存伪**是指实验组和对照组有显著差异，但是我们没有接受新方案。这是第II类错误。

### 新奇效应（Novelty Effect）/首因效应（Primacy Effect）

  用户对新方案B的暂时性好奇或抵触导致行为短期偏离长期趋势。

**量化公式**

-   **首因效应强度** = $\frac{\text{老用户实验组留存率} - \text{老用户对照组留存率}}{\text{新用户实验组留存率} - \text{新用户对照组留存率}}$ （比值\>1.3表示显著存在）

-   **新奇效应衰减率** = $1 - \frac{\text{实验组第7日DAU}}{\text{实验组第2日DAU}}$ （衰减率\>40%需警惕）

解决方案：延长实验时间观察指标是否稳定；考虑仅对新用户实验

-   实验设计阶段

    -   将用户分为 **认知锚点人群**（老用户）和 **无锚点人群**（新用户）

    -   对新用户设置 **学习期**（前3天数据仅监控不分析）

-   数据分析阶段

```{python}
def adjust_novelty_effect(data):
    # 剔除前3天的新奇效应数据
    adjusted_data = data[data['day'] > 3] 
    # 计算首因效应修正因子 (基于老用户衰减曲线)
    primacy_factor = fit_decay_curve(old_user_data)
    return adjusted_data * primacy_factor
```

-   决策阶段：

    -   若首因效应主导，需教育用户（如添加新功能引导教程）

    -   若新奇效应主导，关注长期留存曲线（看第30天留存率）

    -   例如：“当实验组在第4周留存率比对照组高5%，即使首周数据差也值得发布”

    ::: callout-note
    Q：对于一个新功能实施了ABtest，发现新功能效果 更好，但是一周之后效果迅速下降。

    A：新奇效应，效果消失后重复使用减少
    :::

### 样本比例不平衡（Sample Ratio Mismatch - SRM）

  实际分配的人数比例与设计比例（例如50:50）存在显著偏差。严重！

可能的原因：

-   随机化算法bug

-   数据管道问题

-   用户重复计数

-   实验配置错误

错误的例子：

-   实验中，在不同渠道/应用市场中，发布不同版本的APP/页面，并把用户数据进行对比

-   简单地从总体流量中抽取n%用于实验，不考虑流量分布，不做分流处理

  不同应用市场渠道的用户常常带有自己的典型特征，用户分布具有明显区别。对总流量进行简单粗暴地抽样也有着同样的问题——分流到实验组和对照组的流量可能存在很大的分布差异。

  AB测试要求我们尽可能地保持实验组和对照组的流量分布一致，与总流量也需分布一致，否则得出的实验数据不具有可信度。

  **必须解决SRM才能分析结果！**

### 辛普森悖论（Simpson's Paradox）

  在子群体中观察到的趋势与合并数据中的趋势相反。

### 相关实验的相互影响

  前面有讲到，相关性实验放到同一层，不相关的实验可以放在不同层。比如按钮红色or蓝色VS按钮圆形or方形是相关的实验，用户将收到“按钮颜色Red”以及“按钮形状Round”两个策略的影响。

```{mermaid}
graph TD
    U[用户] -->|同时经历| E1[颜色实验]
    U -->|同时经历| E2[形状实验]
    E1 -->|红色/蓝色| D[点击行为]
    E2 -->|圆形/方形| D
    D -->|混杂结果| R[无法区分影响因素]
```

  这样我们就无法判断究竟是哪个策略影响了该用户的行为。换句话说，由于两个实验存在关联，用户重复被实验命中，实验结果实际收到了多个策略的影响。这种情况下，两个实验的结果便不再可信了。

-   传统正交分层（适合不相关实验）

| 用户ID | 层1: 按钮颜色 | 层2: 推荐算法 |
|--------|---------------|---------------|
| 1001   | 蓝色          | 算法A         |
| 1002   | 红色          | 算法B         |
| 1003   | 蓝色          | 算法A         |

-   因子设计（2×2实验，解决相关问题）

| 用户ID | 颜色+形状组合 | 效果评估 |
|--------|---------------|----------|
| 1001   | 蓝色 + 方形   | 转化率X% |
| 1002   | 蓝色 + 圆形   | 转化率Y% |
| 1003   | 红色 + 方形   | 转化率Z% |
| 1004   | 红色 + 圆形   | 转化率W% |

-   数据分析结果：

| 组合方案 | 用户数 | 转化率 | 平均订单额 |
|----------|--------|--------|------------|
| 蓝色方形 | 10000  | 5.2%   | \$128      |
| 蓝色圆形 | 10000  | 6.1%   | \$135      |
| 红色方形 | 10000  | 5.8%   | \$132      |
| 红色圆形 | 10000  | 7.5%   | \$145      |

-   效应计算

```{python}
import numpy as np

# 定义各组转化率
blue_square = 0.052
blue_circle = 0.061
red_square = 0.058
red_circle = 0.075

# 计算主效应和交互效应
color_effect = ((red_square + red_circle) - (blue_square + blue_circle)) / 2
shape_effect = ((blue_circle + red_circle) - (blue_square + red_square)) / 2
interaction = ((red_circle - red_square) - (blue_circle - blue_square)) / 2

print(f"颜色主效应: {color_effect:.4f}")    # 0.0100
print(f"形状主效应: {shape_effect:.4f}")    # 0.0130
print(f"交互效应: {interaction:.4f}")       # 0.0040
```

-   适用场景

| 场景类型     | 案例                   | 推荐设计          |
|--------------|------------------------|-------------------|
| UI元素交互   | 按钮形状+颜色+文案     | 2\*2\*2因子设计   |
| 定价策略组合 | 基础价+折扣+运费策略   | 三因子混合设计    |
| 产品功能叠加 | 搜索+筛选+排序功能优化 | 分阶段因子设计    |
| 营销组合策略 | 渠道+文案+优惠券类型   | 正交分层+因子设计 |

-   实施注意事项：

    -   四组实验需要四倍于单实验的样本量

    -   正交互效应是策略相互增强，负交互效应是策略相互抵消

    -   业务优先级：

        -   当交互效应\>主效应时，必须采用组合策略

        -   当主效应\>\>交互效应时，可以独立优化

    -   实验平台支持：

        ```{python}
        class FactorialExperiment:
            def __init__(self, factors):
                self.factors = factors  # 如 {'颜色': ['蓝','红'], '形状': ['方','圆']}
                self.groups = self._generate_groups()
                
            def _generate_groups(self):
                """生成所有因子组合"""
                from itertools import product
                return list(product(*self.factors.values()))
            
            def assign_user(self, user_id):
                """分配用户到特定组合"""
                hash_val = hash(user_id) % 100
                group_size = 100 / len(self.groups)
                group_idx = int(hash_val // group_size)
                return self.groups[group_idx]
        ```

-   **黄金法则**：当实验策略影响用户的同一决策时刻或同一认知维度时，必须使用因子设计而非正交分层。

### 稀释效应（Dilution Effect）

  指不符合实验触发条件的用户（如从未访问过实验页面的用户纳入分析），稀释了真实效应。

解决方案：

-   采用“仅曝光用户分析”或“触发用户分析”

### 干扰/网络效应（Interference/Network Effects）

  一个用户的体验/行为收到其他用户所在组的影响（如社交功能）。违背了SUTVA（稳定单位处理值假设）。

解决方案：

-   以更大粒度（群组/网络）随机化

-   使用聚类标准误

-   设计特殊实验

### 多重检验问题（Multiple Testing Problem）

  同时检验多个假设或多次查看数据（peeking）会大大增加整体I类错误

解决方案：

-   预先指定核心假设

-   使用序贯检验或贝叶斯方法处理peeking

-   对结果进行多重检验校正

### 长期效应（Long-term Effects）

  AB测试通常运行几天到几周，可能无法捕捉方案的长期影响（如用户疲劳、生态变化）。

解决方案：

-   结合长期跟踪研究

-   使用“保留组”进行长期观察

### 方差估计问题

  这一点在前面也有提到。用户级指标常常存在用户内行为相关性（非独立），导致传统方差估计低估。

解决方法：使用Delta方法、自助法(Bootstrap)、或基于用户聚类的标准误（Cluster-Robust Standard Errors）。对用户级分流尤其关键！

### 贝叶斯AB测试（Bayesian AB Testing）

待补充

### 序贯检验（Sequential Testing）

-   允许在实验运行过程 中多次查看数据并可能提前停止实验（当结果足够显著或明显无效时）

-   能控制整体I类错误率（如使用序贯概率比检验——SPRT，Alpha Spending Functions）

-   提高实验效率（尤其当效应很大或很小），减少资源浪费

### A/B/n Tesing & 多臂老虎机（Multi-armed Bandit - MAB）

-   A/B/n：同时测试多个变体（A/B/C/D...）。需要多重检验校正

-   MAB：一种在线学习算法，在探索（收集信息）和利用（选择当前最优）之间动态平衡流量分配。适用于需要持续优化且快速反馈的场景（如广告竞价）。牺牲严格的因果推断换取更快的收益提升。
